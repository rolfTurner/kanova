\documentclass[12pt]{article}
\parindent 0cm
\usepackage{amssymb,amsmath,graphicx,float}
\usepackage[UKenglish]{isodate}
\usepackage[round]{natbib}
\newcommand{\pullet}{\mbox{\scalebox{0.4}{$\bullet$}}}
\newcommand{\Cov}{\mbox{$\textup{\textrm{Cov}}$}}
\newcommand{\Var}{\mbox{$\textup{\textrm{Var}}$}}
\begin{document}
\cleanlookdateon
\title{Mathematical details of the test statistics used by \texttt{kanova}}
\author{Rolf Turner}
\date{\today}
\maketitle

\section{Introduction}
\label{sec:intro}
This note presents in some detail the formulae for the test statistics
used by the \texttt{kanova()} function from the \texttt{kanova}
package.  These statistics are based on, and generalise, the
ideas discussed in \cite{DiggleEtAl2000} and in \cite{Hahn2012}.
They consist of sums of integrals (over the argument $r$ of the
$K$-function) of the usual sort of analysis of variance ``regression'' sums of
squares, down-weighted over $r$ by the estimated variance of the
quantities being squared.  The limits of integration $r_0$ and $r_1$
\emph{could} be specified in the software (e.g. in the related
\texttt{spatstat} function \texttt{studpermu.test()} they can be
specified in the argument \texttt{rinterval}).  However there is
currently no provision for this in \texttt{kanova()}, and $r_0$ and
$r_1$ are taken to be the min and max of the $r$ component of the
\texttt{"fv"} object returned by \texttt{Kest()}.   Usually $r_0$
is 0 and $r_1$ is $1/4$ of the length of the shorter side of the
bounding box of the observation window in question.

There are test statistics for:
\begin{itemize}
\item one-way analysis of variance (one grouping factor),
\item main effects in a two-way (two grouping factors) additive model, and
\item a model with interaction versus an additive model in a two-way
context.
\end{itemize}

Under the null hypothesis of ``no group effect(s)'' the
\emph{underlying} variance function $\sigma^2(r)$ of the $K$-function
estimates is the same in each cell of the model.  However the
variance of the individual $K$-function estimates changes from
pattern to pattern.  We assume that these variances are inversely
proportional to some power of the number of points in the pattern.
Explicitly, let $K_{ij}(r)$ ($i=1, \ldots, g$, $j = 1, \ldots, n_i$)
be the (\emph{estimated}) $K$-function based on pattern $X_{ij}$
in the one-grouping-factor setting, and $K_{ijk}(r)$ ($i = 1,
\ldots, a$, $j = 1, \ldots, b$, $k = 1, \ldots, n_{ij}$) be the
(\emph{estimated}) $K$-function based on pattern $X_{ijk}$ in the
two-grouping-factor setting.  The variances of these quantities
are assumed to be
\begin{align*}
\Var(K_{ij}(r)) &= \frac{\sigma^2(r)}{w_{ij}} \mbox{~(one grouping factor)} \\
\Var(K_{ijk}(r)) &= \frac{\sigma^2(r)}{w_{ijk}} \mbox{~(two grouping factors)}
\end{align*}
where $w_{ij}$ and $w_{ijk}$ are \emph{weights}.  These weights are
specified, in the code of the \texttt{kanova} package, to be
$n_{ij}^{\eta}$ or $n_{ijk}^\eta$, as is appropriate, where
$n_{ij}$ and $n_{ijk}$ are the numbers of points in pattern $X_{ij}$
or pattern $X_{ijk}$ in the one grouping factor and two grouping
factor settings, respectively.  The exponent $\eta$ is specified
by the user as the \texttt{expo} argument of \texttt{kanova()}. This
argument defaults to 2.

\section{Estimating and testing}
\label{sec:estAndTest}

\subsection{Estimates of $K$-functions}
\label{sec:estKfns}

As indicated in section~\ref{sec:estVar}, the observed $K$-function
estimate, based on the $i$th pattern in the $\ell$th group,
is denoted by $K_{\ell i}(r)$.  In this setting the estimated
$K$-function corresponding to the $\ell$th group is a weighted mean
\[
\tilde{K}_\ell(r) = \sum_{j=1}^{n_{\ell}}
                   \frac{w_{\ell j}}{w_{\ell \pullet}} K_{\ell j}(r)
\]
$\ell = 1, \ldots, g$, and the overall estimate of the (``true'')
$K$-function (common to all groups under the null hypothesis)
is likewise a weighted mean

\[
\tilde{K}(r) = \sum_{\ell=1}^g \sum_{j=1}^{n_{\ell}}
             \frac{w_{\ell j}}{w_{\pullet \pullet}} K_{ij}(r)
= \sum_{\ell=1}^g \frac{w_{\ell \pullet}}{w_{\pullet \pullet}} \tilde{K}_\ell(r) \;.
\]
For positive values of $\eta$ (\texttt{expo}; see
section~\ref{sec:intro}) the effect of using weighted means is
to diminish the influence of quantities corresponding to patterns
having few points, and conversely to emphasise the influence of
quantities corresponding to patterns having many points.

In the two grouping factors setting, it is sometimes necessary
to consider the indices $i$ and $j$ explicitly rather than simply
considering the index $\ell$ corresponding to the groups determined
by ``crossing'' the two ``main effect'' factors.  We define:
\begin{align}
\tilde{K}_{ij}(r)        &= \frac{1}{w_{ij \pullet}} \label{eq:Kij}
                            \sum_{k=1}^{n_{ij}} w_{ijk} K_{ijk} \\
\tilde{K}_{i\pullet}(r)  &= \frac{1}{w_{i \pullet \pullet}} \label{eq:Kidot}
                            \sum_{j=1,b} \sum_{k=1}^{n_{ij}} w_{ijk} K_{ijk} \\
\tilde{K}_{\pullet j}(r) &= \frac{1}{w_{\pullet j \pullet}} \label{eq:Kdotj}
                            \sum_{i=1,a} \sum_{k=1}^{n_{ij}} w_{ijk} K_{ijk} \;.
\end{align}
In the foregoing, \eqref{eq:Kij} specifies the $K$-function estimate
corresponding to the group with index pair $(i,j)$, \eqref{eq:Kidot}
specifies the $K$-function estimate corresponding to the union over
$j$ of the groups with index pair $(i,j)$, and \eqref{eq:Kdotj}
specifies the $K$-function estimate corresponding to the union over
$i$ of the groups with index pair $(i,j)$.

\subsection{Fitted values and residuals}
\label{sec:fitres}

The ``significance'' of the tests that are used in the
\texttt{kanova} package is assessed via permutation tests.  The
permutations may be either of the raw data (as in \citealt{Hahn2012})
or of the residuals from the null model.  The residuals are of
course equal to the observations minus the fitted values.  Since the
observations are functions of distance $r$, the fitted values and
residuals are likewise functions of $r$.  In the single grouping
factor setting we denote the residuals by $R_{ij}(r)$.  In the two
grouping factor setting we denote the residuals by $R_{ijk}(r)$.

There are three scenarios to consider:
\begin{enumerate}
\item Single grouping factor:  Here the null model is the ``mean
only'' model, so the fitted values are all $\tilde{K}(r)$. The
residuals are thus
\[
R_{ij}(r) = K_{ij}(r) - \tilde{K}(r) \; .
\]
\item Two grouping factors, additive model, testing for the significance of
factor A allowing for factor B:   Here the null model is ``no A effect''
so the fitted values are the B effect means.  I.e. the fitted values corresponding
to observations from group $j$ (the group of observations where the level of
B is $j$) are $\tilde{K}_{i\pullet}(r)$.  The residuals are thus
\[
R_{ijk}(r) = K_{ijk}(r) - \tilde{K}_{i\pullet}(r) \; .
\]
\item Two grouping factors, full model, testing for the significance of
interaction between A and B:   Here the null model is the additive model,
so the fitted values corresponding to observations from group $(i,j)$
(where the level of A is $i$ and the level of B is $j$) is $\tilde{K}_{i\pullet}(r)
+ \tilde{K}_{\pullet j}(r) - \tilde{K}(r)$.  The residuals are thus
\[
R_{ijk}(r) = K_{ijk}(r) - \tilde{K}_{i\pullet}(r) - \tilde{K}_{\pullet j}(r)
                        + \tilde{K}(r) \; .
\]
\end{enumerate}

\subsection{Estimating the variance}
\label{sec:estVar}
Under the null hypothesis of ``no group effect(s)'' the (unique)
variance function $\sigma^2(r)$ may be estimated by
\begin{equation}
\label{eq:estvarfun}
s^2(r) = \frac{1}{n_{\pullet}-g} \sum_{\ell = 1}^g \sum_{i=1}^{n_\ell} w_{\ell i}
         \left ( K_{\ell i}(r) - \tilde{K}_{\ell \pullet(r)} \right )^2
\end{equation}
where $g$ is the number of groups, $n_{\ell}$ is the number of
patterns in the $\ell$th group and $w_{\ell i}$ is (proportional to)
the number of points in the $i$th pattern in the $\ell$th group.  The
function $K_{\ell i}(r)$ is the estimated $K$-function corresponding
to the $i$th pattern in the $\ell$th group, and $\tilde{K}_{\ell
\pullet(r)}$ is the ``overall'' estimate of the $K$-function for the
$\ell$th group.  More detail is given in section~\ref{sec:estKfns}.

In the one grouping factor setting, the number of groups $g$ is
the number of levels of the (single) grouping factor.  In the two
grouping factor setting $g = a \times b$ where $a$ is the number
of levels of the first grouping factor, and $b$ is the number of
levels of the second grouping factor.  The quantity $s^2(r)$,
an unbiased estimate of $\sigma^2(r)$, is related to the error
sum of squares that appears in ``ordinary'' analysis of variance,
but the error sum of squares plays no explicit role in the tests
used in the current context.

For completeness we give, below, explicit expressions for $s^2(r)$
in the single grouping factor (one-way) setting, and in the two
grouping factor (two-way) setting:
\[
s^2(r) = \frac{1}{n_{\pullet}-g} \sum_{i = 1}^g \sum_{j=1}^{n_i} w_{ij}
         \left (K_{ij}(r) - \tilde{K}_i(r) \right )^2 \; .
\]
where $n_i$ is the number of patterns in the $i$th group
(one-way), and
\[
s^2(r) = \frac{1}{n_{\pullet \pullet} - ab} \sum_{i = 1}^a \sum_{j=1}^b
         \sum_{k=1}^{n_{ij}} w_{ijk}
         \left (K_{ijk}(r) - \tilde{K}_{ij}(r) \right )^2
\]
where $n_{ij}$ is the number of patterns in the $(i,j)$th group
(two-way).  Clearly the foregoing two equations are special cases
of \eqref{eq:estvarfun}.

\subsection{Testing for the effect of a single grouping factor; one-way anova}
\label{sec:oneway}

The test statistic $T$ is the integral over $r$, of the Studentized
regression sum of squares
\[
T = \sum_{\ell=1}^g n_\ell \int_{r_0}^{r_1} (\tilde{K}_{\ell}(r) -
                 \tilde{K}(r))^2/V_{\ell}(r) \; dr
\]
where $V_{\ell}(r)$ is the (sample) variance of $\tilde{K}_{\ell}(r) - \tilde{K}(r)$.
This variance is given by
\[
V_{\ell}(r) = s^2(r) \left ( \frac{1}{w_\ell \pullet} - \frac{1}{w_{\pullet \pullet}} \right )
\]
where $s^2(r)$ is the overall sample variance given by \eqref{eq:estvarfun}.

\subsection{Testing for main effects in an additive model; two-way anova}
\label{sec:twoway}
In this section we are concerned with testing for a main effect in an
additive model, \emph{allowing} for the possibility of there being a
second main effect.  The test statistics, in the setting of a two-way
additive model, are in effect the same as the test statistics used
in section~\ref{sec:oneway}.  and differ only in their superficial
appearance, the difference being due to the double indexing of
the groups.

We denote the two grouping factors (main effects) by A and B.  The
$K$ functions $K_{\ell }(r)$ are assumed (under the null hypothesis)
to have variance equal or proportional to $\sigma^2(r)/w_{\ell j }$
where $w_{\ell j}$ is the number of points in pattern $X_{\ell j}$,
the $j$th pattern in the $\ell$th group.

In ``ordinary'' analysis of variance we test for an effect of
factor A, allowing for a second factor B, by first calculating the
regression sum of squares
\[
SSA = \sum_{i=1}^a n_i \times (\bar{O}_{i \pullet \pullet} -
                                     \bar{O}_{\pullet \pullet \pullet})^2
\]
where $O_{ijk}$, $k = 1, \ldots, n_i$, are the observations in
cell $(i,j)$ of the model.   Note that in calculating $SSA$ we
are in effect ignoring factor B completely and proceeding as if A
were the only classification factor.  Allowance is made for the B
classification factor by means of the way that the \emph{error}
sum of squares $SSE$ is calculated.  This error sum of squares
\emph{does} involve factor B (and the calculations are based on the
assumption of an additive model).  Testing for the ``significance''
of factor A involves the comparison of $SSA$ with $SSE$.

In the context of the analysis of $K$-function, no error sum of
squares is explicitly used.  Testing for ``significance'' of factor
A is done by means of permutation tests.  If tests are effected
by permuting the raw data, then allowance for factor B is made by
permuting the data \emph{within} each level of B.  I.e. data from
different levels of B do not get swapped with each other.  If tests
are effected by permuting residuals, then allowance for factor B
is made by using residuals from the additive model (and \emph{not}
from the A-only model.

The test statistic, $T_A$, for testing for A (allowing for B) in
the two factor context, is the integral, over $r$ of the Studentized
regression sum of squares.  It is given by
\[
T_A = \sum_{i=1}^a n_{i \pullet} \int_{r_0}^{r_1} (\tilde{K}_{i \pullet}(r) -
                 \tilde{K}(r))^2/V_{Ai}(r) \; dr
\]
where $V_{Ai}(r)$ is the (sample) variance of $\tilde{K}_{i \pullet}(r) - \tilde{K}(r)$.
The function $V_{Ai}(r)$ is given by
\[
V_{Ai}(r) = s^2(r) \left ( \frac{1}{w_{i \pullet \pullet}} -
                           \frac{1}{w_{\pullet \pullet \pullet}} \right )
\]
where $s^2(r)$ is the overall sample variance given by \eqref{eq:estvarfun}.

\subsection{Testing for interaction; two-way anova}
\label{sec:interac}
Here the test statistic is
\[
T_{AB}= \sum_{i=1}^a \sum_{j=1}^b n_{ij} \int_{r_0}^{r_1} (\tilde{K}_{ij}(r) -
   \tilde{K}_{i\pullet}(r) - \tilde{K}_{\pullet j}(r) +
   \tilde{K}(r))^2/V_{ABij}(r) \; dr
\]
where $V_{ABij}(r)$ is the (sample) variance of $\tilde{K}_{ij}(r)
- \tilde{K}_{i\pullet}(r) - \tilde{K}_{\pullet j}(r) + \tilde{K}(r)$.
The function $V_{ABij}(r)$ is given by
\begin{equation}
\label{eq:varSSR}
V_{ABij}(r) = s^2(r) \left ( \frac{1}{w_{i j \pullet}} - \frac{1}{w_{i \pullet \pullet}} +
\frac{2 w_{i j \pullet}}{w_{i \pullet \pullet} w_{\pullet j \pullet}}
- \frac{1}{w_{\pullet j \pullet}} - \frac{1}{w_{\pullet \pullet \pullet}}
\right )
\end{equation}
where, as before, $s^2(r)$ is the overall sample variance
given by \eqref{eq:estvarfun}.  (See Appendix I.)

\bibliographystyle{plainnat}
\bibliography{kanova}

\newpage
\begin{center}
{\LARGE \textbf{Appendix I}}
\end{center}

Here are some (terse) details about the variance of
\mbox{$\tilde{K}_{ij}(r) - \tilde{K}_{i\pullet}(r) - \tilde{K}_{j
\pullet}(r) + \tilde{K}(r)$} as given by \eqref{eq:varSSR}.
\begin{align*}
\Var(\tilde{K}_{ij}(r)) &= \sigma^2/w_{i j \pullet} \\
\Var(\tilde{K}_{i\pullet}(r)) &= \sigma^2/w_{i \pullet \pullet} \\
\Var(\tilde{K}_{\pullet j}(r)  &= \sigma^2/w_{\pullet j \pullet} \\
\Var(\tilde{K}(r)) &= \sigma^2/w_{\pullet \pullet \pullet} \\
\Cov(\tilde{K}_{ij}(r),\tilde{K}_{i \pullet}) &= \sigma^2/w_{i \pullet \pullet} \\
\Cov(\tilde{K}_{ij}(r),\tilde{K}_{\pullet j}) &= \sigma^2/w_{\pullet j \pullet} \\
\Cov(\tilde{K}_{ij}(r),\tilde{K}) &= \sigma^2/w_{\pullet \pullet \pullet} \\
\Cov(\tilde{K}_{i \pullet}(r),\tilde{K}_{\pullet j}) &= w_{i j \pullet}
                \sigma^2/w_{i \pullet \pullet} w_{\pullet j \pullet} \\
\Cov(\tilde{K}_{i \pullet}(r),\tilde{K}) &= \sigma^2/w_{\pullet \pullet \pullet} \\
\Cov(\tilde{K}_{\pullet j}(r),\tilde{K}) &= \sigma^2/w_{\pullet \pullet \pullet}
\end{align*}
Sample calculation:  to see that $\Cov(\tilde{K}_{ij}(r),\tilde{K}_{i
\pullet}) = \sigma^2/w_{i \pullet \pullet}$, note that the two
expressions are weighted sums (with weights $w_{ijk}/w{ij\pullet}$
and $w_{ijk}/w_{i \pullet \pullet}$ respectively) of estimated $K$
functions $K_{ijk}(r)$.  Since these $K$ functions correspond to
independent patterns, they are likewise independent, and so the
covariances are 0 except where the indices of the terms coincide.
In this case the covariance is the product of the weights and the
variance of the term.  We get
\begin{align*}
\sum_{k=1}^{n_{ij}} \frac{w_{ijk}}{w_{ij\pullet}}
                    \frac{w_{ijk}}{w_{i \pullet \pullet}}
                    \frac{\sigma^2}{w_{ijk}}
&= \frac{\sigma^2}{w_{ij\pullet} w_{i \pullet \pullet}} \sum_{k=1}^{n_ij} w_{ijk} \\
&= \frac{\sigma^2}{w_{ij\pullet} w_{i \pullet \pullet}} w_{ij\pullet} \\
&= \frac{\sigma^2}{w_{i \pullet \pullet}}
\end{align*}
\newpage
The variance term of interest is $\Var(\tilde{K}_{ij}(r) -
\tilde{K}_{i\pullet}(r) - \tilde{K}_{j \pullet}(r) + \tilde{K}(r))$
which is equal to
\begin{equation}
\label{eq:var}
\begin{split}
& \Var(\tilde{K}_{ij}(r)) +
\Var(\tilde{K}_{i\pullet}(r)) +
\Var(\tilde{K}_{\pullet j}(r)) +
\Var(\tilde{K}(r)) \\
& -2 \Cov(\tilde{K}_{ij}(r),\tilde{K}_{i \pullet})
-2 \Cov(\tilde{K}_{ij}(r),\tilde{K}_{\pullet j}) +
2 \Cov(\tilde{K}_{ij}(r),\tilde{K}) \\ 
& +2 \Cov(\tilde{K}_{i \pullet}(r),\tilde{K}_{\pullet j}) -
2 \Cov(\tilde{K}_{i \pullet}(r),\tilde{K})\\
& - 2 \Cov(\tilde{K}_{\pullet j}(r),\tilde{K}) \; .
\end{split}
\end{equation}
Using the previously stated expressions for the variances and covariances
of the component terms, we see that \eqref{eq:var}
is equal to
\[
\sigma^2 \left (
\frac{1}{w_{i j \pullet}} +
\frac{1}{w_{i \pullet \pullet}} +
\frac{1}{w_{\pullet j \pullet}} +
\frac{1}{w_{\pullet \pullet \pullet}} -
\frac{2}{w_{i \pullet \pullet}} -
\frac{2}{w_{\pullet j \pullet}} +
\frac{2}{w_{\pullet \pullet \pullet}} +
\frac{2 w_{i j \pullet}}{w_{i \pullet \pullet} w_{\pullet j \pullet}} -
\frac{2}{w_{\pullet \pullet \pullet}} -
\frac{2}{w_{\pullet \pullet \pullet}}
\right )
\]
which is finally equal to
\[
\sigma^2 \left (
\frac{1}{w_{i j \pullet}} -
\frac{1}{w_{i \pullet \pullet}} +
\frac{2 w_{i j \pullet}}{w_{i \pullet \pullet} w_{\pullet j \pullet}} -
\frac{1}{w_{\pullet j \pullet}} -
\frac{1}{w_{\pullet \pullet \pullet}}
\right )
\]

\newpage
\begin{center}
{\LARGE \textbf{Appendix II}}
\end{center}

As indicated in Section~\ref{sec:twoway}, the test that is used by
\texttt{kanova()} is based on random permutations either of the data
or of the residuals from an appropriate model).  If the permutations
are of the data, then allowing for the possibility of a second main
effect must be accomplished by permuting the data in such a way
that the second main effect does not mask the first main effect.
That is, the data must be permuted within the levels of the second
main effect.  Here we elaborate a bit on what this means.

To illustrate this idea in as clear and simple manner as possible,
we consider an artificial example of an additive two-factor
\emph{scalar} model with factors A and B, have levels $A_1, A_2,
A_3$ and $A_4$, and $B_1, B_2$ and $B_3$ respectively.  Suppose the
underlying means corresponding to factor A are 0.2, 0.4, 0.6 and 0.8,
and those corresponding to factor B are 0, 5 and 10.  Note that the
B effect is much``larger'' than the A effect and would overwhelm
the A effect unless appropriate steps were taken.

In an additive model the population ``cell means'' are:
\begin{table}[H]
\caption{\label{tab:exmpl}}
\begin{center}
\begin{tabular}{|l|l|l|l|} \hline
      & $B_1$ & $B_2$ & $B_3$ \\ \hline
$A_1$ & 0.2 & 5.2 & 10.2 \\ \hline
$A_2$ & 0.4 & 5.4 & 10.4 \\ \hline
$A_3$ & 0.6 & 5.6 & 10.6 \\ \hline
$A_4$ & 0.8 & 5.8 & 10.8 \\ \hline
\end{tabular}
\end{center}
\end{table}

When we test for an A effect in this example, we look at a model with 12
cells, three of which correspond to level $A_1$, of A, three to level
$A_2$, three to level $A_3$ and three to level $A_4$.  A pseudo
test statistic, i.e. a simplified version of the test statistic
used in genuine analyses, has the form (for the observed data)
\[
T = (5.2 - 5.5)^2 + (5.4 - 5.5)^2 + (5.6 - 5.5)^2 + (5.8 - 5.5)^2 = 0.2
\]
where the 5.2, 5.4, 5.6 and 5.8 terms in the foregoing are the
means corresponding to the levels of A, 5.5 is the ``grand mean'',
and where we ignore the ``noise'' that would appear in any real data.

In conducting a test for an A effect we compare the test statistic
from the observed data with test statistics $T_i^*$ formed from
permutations of the observed data.  Since there \emph{is} an A
effect, we would hope that the test would reject the null hypothesis,
i.e. that $T$ would be large compared with the bulk of the $T_i^*$.

This will happen if we permute the data ``within the levels
of B'', i.e. if we permute, separately, each of the columns of
Table~\ref{tab:exmpl}.  If we permute the data in this manner,
then the $T_i^*$ that are produced are all small relative to $T$.
In fact, in this particular (artificial) example, it is possible
to enumerate all 13824 values of $T_i^*$ that arise from permuting
the data within the levels of B.  These are all less than or equal
to $T = 0.2$.

$T_i^*$, that arise from permuting the data within the levels of B,
are less than or equal to $T = 0.2$.

However if we fail to permute the data within the levels of B,
then from time to time large values will be grouped together, within
a level of factor A, with other large values.  This phenomenon
results in the creation of means, for one or more levels $A_i$ of
factor A, which are very different from the overall mean, resulting
in large contributions to the calculated statistic.  For instance
an arbitrary permutation of the 12 data values might result in
\begin{table}[H]
\caption{\label{tab:arbperm}}
\begin{center}
\begin{tabular}{|l|l|l|l|l|} \hline
\multicolumn{4}{|c}{} & \multicolumn{1}{|c|}{mean} \\ \hline
$A_1$ & 10.4 & 10.6 &  5.8 & 8.9333 \\ \hline
$A_2$ &  0.8 & 10.8 &  0.2 & 3.9333 \\ \hline
$A_3$ &  5.6 &  5.2 &  0.6 & 3.8000 \\ \hline
$A_4$ &  0.4 &  5.4 & 10.2 & 5.3333 \\ \hline
\end{tabular}
\end{center}
\end{table}
The value of the pseudo test statistic obtained from the
data in Table~\ref{tab:arbperm}, is
\[
(8.9333 - 5.5)^2 + (3.9333 - 5.5)^2 + (3.8000 - 5.5)^2 + (5.3333 - 5.5)^2
= 17.16
\]
which is much larger than the pseudo test statistic from the observed
data shown in Table~\ref{tab:exmpl}.  Generally, the values of
the $T_i^*$ resulting from arbitrary permutations will be large in
comparison with $T$ and the null hypothesis will not be rejected
as it should be.

\end{document}
